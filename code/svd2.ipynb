{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['duration (ms)', 'danceability', 'energy', 'loudness', 'speechiness',\n",
      "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
      "       'spec_rate', 'labels', 'uri', 'user_id', 'group_no'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "user_data = pd.read_csv('../datasets/simulated_user_dataset.csv')\n",
    "user_data = user_data.drop('Unnamed: 0', axis=1)\n",
    "print(user_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qd/k17l2jvs6p31s4x32lk2fgc00000gn/T/ipykernel_13136/1554311331.py\", line 3, in <module>\n",
      "    from surprise import Dataset, Reader, SVD\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/surprise/__init__.py\", line 6, in <module>\n",
      "    from .prediction_algorithms import (\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n",
      "    from .algo_base import AlgoBase\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n",
      "    from .. import similarities as sims\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, SVD\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_validate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/__init__.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset_dir\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction_algorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     AlgoBase,\n\u001b[1;32m      8\u001b[0m     BaselineOnly,\n\u001b[1;32m      9\u001b[0m     CoClustering,\n\u001b[1;32m     10\u001b[0m     KNNBaseline,\n\u001b[1;32m     11\u001b[0m     KNNBasic,\n\u001b[1;32m     12\u001b[0m     KNNWithMeans,\n\u001b[1;32m     13\u001b[0m     KNNWithZScore,\n\u001b[1;32m     14\u001b[0m     NMF,\n\u001b[1;32m     15\u001b[0m     NormalPredictor,\n\u001b[1;32m     16\u001b[0m     Prediction,\n\u001b[1;32m     17\u001b[0m     PredictionImpossible,\n\u001b[1;32m     18\u001b[0m     SlopeOne,\n\u001b[1;32m     19\u001b[0m     SVD,\n\u001b[1;32m     20\u001b[0m     SVDpp,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainset\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/prediction_algorithms/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`prediction_algorithms` package includes the prediction algorithms\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mavailable for recommendation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    co_clustering.CoClustering\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgo_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlgoBase\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline_only\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaselineOnly\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mco_clustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoClustering\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/prediction_algorithms/algo_base.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`surprise.prediction_algorithms.algo_base` module defines the base\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mclass :class:`AlgoBase` from which every single prediction algorithm has to\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03minherit.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m similarities \u001b[38;5;28;01mas\u001b[39;00m sims\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize_baselines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m baseline_als, baseline_sgd\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prediction, PredictionImpossible\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/similarities.pyx:1\u001b[0m, in \u001b[0;36minit surprise.similarities\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prepare_data_for_svd(df):\n",
    "    \"\"\"\n",
    "    Prepare the data for SVD by creating a user-item interaction matrix\n",
    "    with normalized features.\n",
    "    \"\"\"\n",
    "    # Select numerical features for creating the interaction value\n",
    "    feature_columns = [\n",
    "        'danceability', 'energy', 'loudness', 'speechiness',\n",
    "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "        'duration (ms)', 'spec_rate', 'labels'\n",
    "    ]\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(df[feature_columns]),\n",
    "        columns=feature_columns\n",
    "    )\n",
    "    \n",
    "    # Create an interaction score (you can modify this based on your needs)\n",
    "    df_normalized['interaction_score'] = df_normalized.mean(axis=1)\n",
    "    \n",
    "    # Create Surprise reader object\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    \n",
    "    # Prepare data for Surprise\n",
    "    data = Dataset.load_from_df(\n",
    "        df[['user_id', 'uri', 'interaction_score']],\n",
    "        reader\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_svd_model(data, n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02):\n",
    "    \"\"\"\n",
    "    Train an SVD model with the given parameters.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train SVD model\n",
    "    model = SVD(\n",
    "        n_factors=n_factors,\n",
    "        n_epochs=n_epochs,\n",
    "        lr_all=lr_all,\n",
    "        reg_all=reg_all\n",
    "    )\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    return model, testset\n",
    "\n",
    "def evaluate_model(model, testset):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance on the test set.\n",
    "    \"\"\"\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean([float((pred.r_ui - pred.est) ** 2) for pred in predictions]))\n",
    "    mae = np.mean([float(abs(pred.r_ui - pred.est)) for pred in predictions])\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    }\n",
    "\n",
    "def get_recommendations(model, user_id, df, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Get top N song recommendations for a specific user.\n",
    "    \"\"\"\n",
    "    # Get all unique songs\n",
    "    all_songs = df['uri'].unique()\n",
    "    \n",
    "    # Get songs the user hasn't interacted with\n",
    "    user_songs = set(df[df['user_id'] == user_id]['uri'])\n",
    "    songs_to_predict = list(set(all_songs) - user_songs)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = [model.predict(user_id, song_id) for song_id in songs_to_predict]\n",
    "    \n",
    "    # Sort predictions by estimated rating\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "    \n",
    "    # Return top N recommendations\n",
    "    return predictions[:n_recommendations]\n",
    "\n",
    "# Example usage\n",
    "def main(df):\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the recommendation system.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    data = prepare_data_for_svd(df)\n",
    "    \n",
    "    # Train model\n",
    "    model, testset = train_svd_model(data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, testset)\n",
    "    print(\"Model Performance:\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"MAE: {metrics['MAE']:.4f}\")\n",
    "    \n",
    "    # Get recommendations for a sample user\n",
    "    sample_user = df['user_id'].iloc[0]\n",
    "    recommendations = get_recommendations(model, sample_user, df)\n",
    "    \n",
    "    return model, recommendations\n",
    "\n",
    "# Cross-validation function for parameter tuning\n",
    "def perform_cross_validation(data, param_grid):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to find the best parameters.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_factors in param_grid['n_factors']:\n",
    "        for n_epochs in param_grid['n_epochs']:\n",
    "            for lr in param_grid['lr_all']:\n",
    "                model = SVD(\n",
    "                    n_factors=n_factors,\n",
    "                    n_epochs=n_epochs,\n",
    "                    lr_all=lr\n",
    "                )\n",
    "                \n",
    "                cv_results = cross_validate(model, data, measures=['RMSE', 'MAE'], \n",
    "                                         cv=5, verbose=False)\n",
    "                \n",
    "                results.append({\n",
    "                    'n_factors': n_factors,\n",
    "                    'n_epochs': n_epochs,\n",
    "                    'lr_all': lr,\n",
    "                    'RMSE': cv_results['test_rmse'].mean(),\n",
    "                    'MAE': cv_results['test_mae'].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
